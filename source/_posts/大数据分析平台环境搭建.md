---
title: 分布式大数据分析平台环境搭建
tags:
  - 大数据
  - cuda
  - caffe
  - opencv
  - 深度学习
  - 分布式
categories:
  - 解决方案
  - hadoop
date: 2017-03-07 10:29:58
---

## 安装ubuntu
在windows上使用ultraISO软件来刻录Ubuntu系统安装盘，设置U盘启动，安装系统
参考链接：[Ubuntu 14.04 64位系统安装cuda8.0+cudnn7.5+opencv+caffe 血泪教程](https://xiongraorao.github.io/2017/01/14/%E5%8E%9F-Ubuntu-14-04-64%E4%BD%8D%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85cuda8-0-cudnn7-5-opencv-caffe-%E8%A1%80%E6%B3%AA%E6%95%99%E7%A8%8B/)

## 安装NVIDIA驱动
[http://www.nvidia.cn/Download/index.aspx?lang=cn](http://www.nvidia.cn/Download/index.aspx?lang=cn)
选择合适型号，下载驱动包

- 首先按Ctrl+Alt+F1，进入text mode，关闭显示驱动
   sudo service lightdm stop
- 增加执行权限

    ``` bash
    sudo chmod +x NVIDIA-Linux-x86_64-375.39.run
    sudo ./NVIDIA-Linux-x86_64-375.39.run
    ```
- 安装成功后，重启图像界面服务
   sudo service lightdm start

## 安装cuda+cudnn+opencv+caffe
参考这个链接：[原-Ubuntu-14-04-64位系统安装cuda8-0-cudnn7-5-opencv-caffe-血泪教程](https://xiongraorao.github.io/2017/01/14/%E5%8E%9F-Ubuntu-14-04-64%E4%BD%8D%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85cuda8-0-cudnn7-5-opencv-caffe-%E8%A1%80%E6%B3%AA%E6%95%99%E7%A8%8B/)

caffe_github_link:[https://github.com/BVLC/caffe.git](https://github.com/BVLC/caffe.git)
opencv_github_link:[https://github.com/opencv/opencv](https://github.com/opencv/opencv)

## 配置静态ip + 安装ssh-server
### 静态ip
ubuntu14.04设置静态ip

1. 找到文件并作如下修改：

sudo vim /etc/network/interfaces

修改如下部分：

auto eth0
iface eth0 inet static
address 192.168.1.4
gateway 192.168.1.1 #这个地址你要确认下 网关是不是这个地址
netmask 255.255.255.0
#network 192.168.0.0
#broadcast 192.168.0.255

2. 修改dns解析

因为以前是dhcp解析，所以会自动分配dns服务器地址

而一旦设置为静态ip后就没有自动获取到的dns服务器了

要自己设置一个

sudo vim /etc/resolv.conf

写上一个公网的DNS

nameserver 202.114.0.131(公网dns服务器)
nameserver 202.114.0.242（公网dns服务器）
nameserver 192.168.1.1（局域网内可以访问外网的机器，非网关服务器放在前面）

sudo vim /etc/resolvconf/resolv.d/base

nameserver 202.114.0.131(公网dns服务器)
nameserver 202.114.0.242（公网dns服务器）
nameserver 192.168.1.1（局域网内可以访问外网的机器，非网关服务器放在前面）

3. 重启网卡：

sudo /etc/init.d/network restart

### 安装ssh-server
- 判断是否安装ssh服务
Ubuntu系统默认安装ssh-client，如果想远程登录主机，就需要安装ssh-server，如下命令：
    ``` bash
    ps -e|grep sshps -e|grep ssh
    ```
ssh-agent表示ssh-client启动，sshd表示ssh-server启动了

如果缺少sshd，说明ssh服务没有启动或者没有安装。
``` bash
sudo apt-get install openssh-client #安装ssh-client命令
sudo apt-get install openssh-server #安装ssh-server命令
sudo /etc/init.d/ssh start #启动服务
ps -e|grep sshps -e|grep ssh #查看是否正确启动。
```
- 配置ssh服务端口
默认端口是 22
sudo gedit /etc/ssh/sshd_config
sudo /etc/init.d/ssh restart #重启ssh服务生效

- 设置开机自动启动

## 安装storm+kafka+zookeeper+hdfs+hbase+jdk
### 前期准备工作

1. 首先下载好这几个文件，这里用的版本分别是：
- storm ***0.9.6***
- zookeeper ***3.4.0***
- jdk ***1.8.0.92***
- kafka ***2.11.0.10.0.0***
- hadoop ***2.6.4***
- hbase ***1.2.4***

2. 设置ip地址映射
sudo vi /etc/hosts

``` bash
# test
192.168.0.136   pc1080
192.168.0.116   pcwrp
192.168.0.117   pcxrr

192.168.0.136   cloud01
192.168.0.116   cloud02
192.168.0.117   cloud03

192.168.0.136   hadoop01
192.168.0.116   hadoop02
192.168.0.117   hadoop03

192.168.0.136   zk01
192.168.0.116   zk02
192.168.0.117   zk03

192.168.0.136   kafka01
192.168.0.116   kafka02
192.168.0.117   kafka03

192.168.0.136   kafka01
192.168.0.116   kafka02
192.168.0.117   kafka03

192.168.0.136   storm01
192.168.0.116   storm02
192.168.0.117   storm03

192.168.0.136   hbase01
192.168.0.116   hbase02
192.168.0.117   hbase03

# end test
```

<span style="color:red">这里面需要注意一下，之所以需要配置多组ip映射，是因为在相应的软件中很多默认参数需要用到这些映射</span>

3. 三台机机器实现互相ssh免密码登录，并且实现自身环回登录
参考链接：[ssh免密码登录](https://xiongraorao.github.io/2017/02/17/ssh%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95/)

4. 设置环境变量
vi ~/.bashrc
``` bash
# java
export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$JRE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
# hadoop
export MAVEN_HOME=/home/hadoop/cloud/apache-maven-3.3.9/
export ZOOKEEPER_HOME=/home/hadoop/cloud/zookeeper-3.4.9/
export HADOOP_HOME=/home/hadoop/cloud/hadoop-2.6.4/
export HBASE_HOME=/home/hadoop/cloud/hbase-1.2.4/
export FLUME_HOME=/home/hadoop/cloud/apache-flume-1.6.0/
export KAFKA_HOME=/home/hadoop/cloud/kafka_2.11-0.10.0.0/
export STORM_HOME=/home/hadoop/cloud/apache-storm-0.9.6/
export HIVE_HOME=/home/hadoop/cloud/apache-hive-1.2.1/
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_PREFIX=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib/native"
#export CHUKWA_HOME=/home/hadoop/cloud/chukwa-0.8.0/
#export CHUKWA_CONF_DIR=$CHUKWA_HOME/etc/chukwa
PATH=$PATH:$HOME/bin
PATH=$JAVA_HOME/bin/:$PATH
PATH=$MAVEN_HOME/bin/:$PATH
PATH=$ZOOKEEPER_HOME/bin/:$PATH
PATH=$HBASE_HOME/bin/:$PATH
PATH=$FLUME_HOME/bin/:$PATH
PATH=$KAFKA_HOME/bin/:$PATH
PATH=$STORM_HOME/bin/:$PATH
PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
PATH=$HIVE_HOME/bin/:$PATH
PATH=$PATH:$CHUKWA_HOME/bin:$CHUKWA_HOME/sbin
export PATH

```

### 安装zookeeper
- 下载安装包，解压到 /home/hadoop/cloud 目录下
- 添加环境变量
- **配置/etc/hosts 文件，做好ip地址映射**
- cat zoo_sample.cfg >> zoo.cfg
- 编辑 zoo.cfg文件
``` bash
tickTime=2000
initLimit=10
syncLimit=5
# zookeeper数据保存路径
dataDir=/home/hadoop/cloud/zookeeper-3.4.9/data/
clientPort=2181
maxClientCnxns=150
maxSessionTimeout=100000
autopurge.snapRetainCount=6
autopurge.purgeInterval=48
server.1=zk01:2888:3888
server.2=zk02:2888:3888
server.3=zk03:2888:3888

```
- 在节点配置的dataDir目录中创建一个myid文件，里面内容为一个数字，用来标识当前主机，$ZOOKEEPER_HOME/conf/zoo.cfg文件中配置的server.X，则myid文件中就输入这个数字X。（即在每个节点上新建并设置文件myid，其内容与zoo.cfg中的id相对应）
``` bash
cd /home/hadoop/cloud/zookeeper-3.4.9/data/
mkdir myid
vi myid
```

- 启动zookeeper
``` bash
$ZOOKEEPER_HOME/bin/zkServer.sh start
```

### 安装storm
- 下载安装包，解压到 /home/hadoop/cloud 目录下
- 设置环境变量
- 修改 storm 主目录下conf/storm.yaml文件
``` bash
storm.zookeeper.servers:
 - "zk01"
 - "zk02"
 - "zk03"
storm.zookeeper.port: 2181
storm.zookeeper.root: "/storm"
nimbus.host: "storm01"

supervisor.slots.ports:
 - 6700
 - 6701
 - 6702
 - 6703
 - 6704
 - 6705
 - 6706
 - 6707
 - 6708
storm.local.dir: "/home/hadoop/cloud/apache-storm-0.9.6/local"

drpc.servers:
 - "storm01"
drpc.port: 3772
drpc.worker.threads: 100
drpc.queue.size: 1024
drpc.invocations.port: 3773
drpc.request.timeout.secs: 800
drpc.childopts: "-Xmx2048m"
drpc.port: 3772
drpc.worker.threads: 100
drpc.queue.size: 1024
drpc.invocations.port: 3773
drpc.request.timeout.secs: 800
drpc.childopts: "-Xmx2048m"

#JVM options of worker
#edited by persist
superivisor.childopts: "-Xmx2048m"
worker.childopts: "-Xmx2048m"
#JVM options of worker
#edited by persist
superivisor.childopts: "-Xmx2048m"
worker.childopts: "-Xmx2048m"

```

- 启动storm命令（**启动storm之前需要启动zookeeper**）

``` bash
nimubs-host: nohup bin/storm nimbus&
nimubs-host: nohup bin/storm ui&
superivisor-hosts: nohup bin/storm superivisor&
```
- 浏览器输入：[http://pc1080:8080](http://pc1080:8080) 查看storm UI界面

### 安装hadoop
- 下载安装包，解压到 /home/hadoop/cloud 目录下
- 设置环境变量
- 修改主目录下 etc/hadoop-env.sh
``` bash
export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/
```
- 修改主目录下 etc/yarn-env.sh
``` bash
export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/
```

- 修改 etc/hadoop/mapred-env.sh
``` bash
export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/
```

- 修改主目录下 etc/core-site.xml
``` bash
<configuration>
    <property>
            <name>fs.defaultFS</name>
            <value>hdfs://hadoop01:9000</value>
    </property>
    <property>
                <name>hadoop.tmp.dir</name>
                <value>file:/home/hadoop/cloud/hadoop-2.6.4/data/tmp</value>
                <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>zk01:2181,zk02:2181,zk03:2181</value>
        <description>这里是ZooKeeper集群的地址和端口。注意，数量一定是奇数，且不少于三个节点</description>
    </property>
    # add ours here
    <property>
        <name>hadoop.proxyuser.hadoop.groups</name>
        <value>*</value>
        <description>允许所有用户组用户代理</description>
    </property>
    <property>
        <name>hadoop.proxyuser.hadoop.hosts</name>
        <value>hadoop01</value>
        <description>允许挂载的主机域名</description>
    </property>
</configuration>

```
- 修改主目录下 etc/hdfs-site.xml
``` bash
<configuration>
        <property>
                <name>dfs.nfs.exports.allowed.hosts</name>
                <value>hadoop01 rw;hadoop02 rw;hadoop03 rw;</value>
        </property>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>hadoop01:50090</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>2</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:/home/hadoop/cloud/hadoop-2.6.4/tmp/dfs/name</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:/home/hadoop/cloud/hadoop-2.6.4/tmp/dfs/data</value>
        </property>
		<property>
                <name>dfs.webhdfs.enabled</name>
                <value>true</value>
		</property>

        ## add options here
		<property>
				<name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
				<value>true</value>
		</property>
		<property>
                <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
		        <value>NEVER</value>
		</property>
		<property>
                <name>dfs.namenode.fs-limits.max-component-length</name>
                <value>255</value>
                <description>Defines the maximum number of bytes in UTF-8 encoding in each
                    component of a path.  A value of 0 will disable the check.</description>
        </property>
        <property>
                <name>dfs.namenode.fs-limits.max-directory-items</name>
                <value>6000000</value>
                <description>Defines the maximum number of items that a directory may
                    contain. Cannot set the property to a value less than 1 or more than
                    6400000.</description>
        </property>
        <property>
                <name>dfs.namenode.fs-limits.min-block-size</name>
                <value>1048576</value>
                <description>Minimum block size in bytes, enforced by the Namenode at create
                    time. This prevents the accidental creation of files with tiny block
                    sizes (and thus many blocks), which can degrade performance.</description>
        </property>
        <property>
                <name>dfs.namenode.fs-limits.max-blocks-per-file</name>
                <value>1048576</value>
                <description>Maximum number of blocks per file, enforced by the Namenode on
                    write. This prevents the creation of extremely large files which can
                    degrade performance.</description>
        </property>
        <property>
                <name>dfs.namenode.fs-limits.max-xattrs-per-inode</name>
                <value>32</value>
                <description>
                    Maximum number of extended attributes per inode.
                </description>
        </property>
        <property>
                <name>dfs.namenode.fs-limits.max-xattr-size</name>
                <value>16384</value>
                <description>
                    The maximum combined size of the name and value of an extended attribute
                    in bytes. It should be larger than 0, and less than or equal to maximum
                    size hard limit which is 32768.
                </description>
       </property>
	   <property>
                <name>dfs.datanode.max.transfer.threads</name>
                <value>8192</value>
      </property>
</configuration>

```

- 修改主目录下 etc/httpfs-env.sh
``` bash
export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/

export HTTPFS_HOME=/home/hadoop/cloud/hadoop-2.6.4/

export HTTPFS_CONFIG=/home/hadoop/cloud/hadoop-2.6.4/etc/hadoop

export CATALINA_BASE=/home/hadoop/cloud/hadoop-2.6.4/share/hadoop/httpfs/tomcat

export HTTPFS_LOG=${HTTPFS_HOME}/httpfs/logs


export HTTPFS_TEMP=${HTTPFS_HOME}/httpfs/temp


export HTTPFS_HTTP_PORT=14000


export HTTPFS_ADMIN_PORT=`expr ${HTTPFS_HTTP_PORT} + 1`

```

- 修改 etc/hadoop/slavers

``` bash
hadoop01
hadoop02
hadoop03
```

<span style="color:red">以上只是部分配置，其中hadoop01是默认的主节点，默认配置的主机</span>

- 启动hadoop
  - 首先需要启动zookeeper
  - 格式化hdfs： bin/hdfs namenode -format
  - 启动hdfs和yarn： sbin/start-all.sh,启动成功master出现NameNode，SecondaryNameNode，ResourceManger进程，slave节点出现DataNode和NodeManager
**注意事项：如果增加了多个ip映射，那么有可能提醒是否需要添加host到ssh表，这种情况框需要重新设置ssh互通**

- web查看
hdfs: https://hadoop01:50070/
yarn: https://hadoop01:8088/

遇到的问题：
主节点上启动start-all.sh, 只有主节点的NameNode和SecondaryNameNode和DataNode启动，从节点的DataNode没有启动

重启部分坏死节点：
bin/Hadoop-daemon.sh start DataNode
bin/Hadoop-daemon.sh start jobtracker

动态加入新节点：
bin/Hadoop-daemon.sh --config ./conf start DataNode
bin/Hadoop-daemon.sh --config ./conf start tasktracker

更新配置文件命令：
``` bash
for ip in `seq 2 3`; do scp /home/hadoop/cloud/hadoop-2.6.4/etc/hadoop/* hadoop@hadoop0$ip:/home/hadoop/cloud/hadoop-2.6.4/etc/hadoop/; done
```

更新环境变量：
``` bash
for ip in `seq 2 3`; do scp /home/hadoop/.bashrc hadoop@hadoop0$ip:/home/hadoop/; done
```

### 安装HBase
- 下载安装包，解压到 /home/hadoop/cloud 目录下
- 修改～/.bashrc文件，添加HBASE_HOME环境变量；
- 修改 conf/hbase-env.sh
``` bash
export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/
export HBASE_CLASSPATH=/opt/hbase/conf  (extral classpath like java, Optional)
# 此配置信息，设置由zk集群管理，故为false  
export HBASE_MANAGES_ZK=false  
export HBASE_HOME=/opt/hbase  (Optional)
#Hbase日志目录  
export HBASE_LOG_DIR=$HBASE_HOME/logs(Optional)
```
- 修改 conf/hbase-site.xml
``` bash
<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://hbase01:9000/hbase</value>
        <description>The directory shared by region servers.</description>
    </property>
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
        <description>Property from ZooKeeper config zoo.cfg. The port at which the clients will connect.
        </description>
    </property>
    <property>
        <name>hbase.master.info.bindAddress</name>
        <value>192.168.100.101</value>
        <description>HBase Master web ui bind address</description> **必须填写master节点地址**
    </property>
    <property>
        <name>zookeeper.session.timeout</name>
        <value>60000</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>hbase01,hbase02,hbase03</value>
    </property>
    <property>
        <name>hbase.tmp.dir</name>
        <value>/home/hadoop/cloud/hbase-1.2.4/data/tmp</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.dataDir</name>
        <value>/home/hadoop/cloud/hbase-1.2.4/zookeeper</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.maxClientCnxns</name>
        <value>300</value>
    </property>
    <property>
        <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
        <value>NEVER</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

- 修改 regionservers
``` bash
hbase01
hbase02  #设置regionserver的节点
hbase03
```
- 同步配置
``` bash
for ip in `seq 2 3`; do scp /home/hadoop/cloud/hbase-1.2.4/conf/* hadoop@hadoop0$ip:/home/hadoop/cloud/hbase-1.2.4/conf/; done
```

- 启动hbase
``` bash
bin/start-hbase.sh
启动后，jps可以看到有这两个进程
HRegionServer
HMaster
```
- 启动Hbase Shell
``` bash
bin/hbase shell
```
- 访问HBase UI
查看hbase管理界面http://192.168.181.66:16010

- hbase日常维护
参考链接：[Hbase基本操作命令](https://xiongraorao.github.io/2017/03/09/Hbase%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/#more)

> <span style='color:red;font-size: 26px;font-weight: bold'>注意hbase集群安装的大坑</span>
> 1. hbase-site.xml必须添加hbase.master.info.bindAddress配置项，设置为master节点的ip地址
> 2. 设置/etc/hosts文件，注释掉 127.0.1.1 hostname这一行，替换成局域网ip hostname，否则HRegionServer 无法链接Master节点

错误参考链接：[http://www.cnblogs.com/colorfulkoala/archive/2012/07/09/2583841.html](http://www.cnblogs.com/colorfulkoala/archive/2012/07/09/2583841.html)

### 安装kafka
参考链接：[kafka](http://121.42.164.108/index.php/2017/03/08/ubuntu14-04-stormkafkazookeeperhdfs/)
