---
layout: java
title: 深度学习(1)
date: 2018-10-25 09:27:37
tags: 
  - deeplearning
  - 深度学习
mathjax: true
---

<!-- TOC -->

- [概要](#概要)
- [卷积神经网络（CNN）](#卷积神经网络cnn)
    - [卷积层](#卷积层)
    - [DropOut](#dropout)
    - [激活层](#激活层)
    - [正则化层](#正则化层)

<!-- /TOC -->

本系列日志为深度学习的笔记

# 概要

深度学习

# 卷积神经网络（CNN）

## 卷积层

## DropOut

为了提高CNN的特征表达能力，通常采用更深的网络和更多的神经元，但是复杂的网络容易导致过拟合，利用Dropout在一定程度上面可以防止过拟合

![](https://images2015.cnblogs.com/blog/604152/201703/604152-20170330212327961-771136311.jpg)

如上图左，为没有Dropout的普通2层全连接结构，记为 r=a(Wv)，其中a为激活函数。

如上图右，为在第2层全连接后添加Dropout层的示意图。即在 模 型 训 练 时 随机让网络的某些节点不工作（输出置0），其它过程不变。

参考链接：
- [系列解读Dropout](https://www.cnblogs.com/welhzh/p/6648613.html)


## 激活层

Sigmoid 函数：$S(x) = \frac{1}{1+e^{-x}}$

## 正则化层

批量正则化:

$$ y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta $$

