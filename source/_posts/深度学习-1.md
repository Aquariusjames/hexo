---
layout: java
title: 深度学习(1)
date: 2018-10-25 09:27:37
tags: 
  - deeplearning
  - 深度学习
mathjax: true
---

<!-- TOC -->

- [概要](#概要)
- [卷积神经网络（CNN）](#卷积神经网络cnn)
    - [卷积层](#卷积层)
    - [DropOut](#dropout)
    - [激活层](#激活层)
    - [正则化层](#正则化层)
    - [Triplet Loss](#triplet-loss)

<!-- /TOC -->

本系列日志为深度学习的笔记

# 概要

深度学习

# 卷积神经网络（CNN）

## 卷积层

## DropOut

为了提高CNN的特征表达能力，通常采用更深的网络和更多的神经元，但是复杂的网络容易导致过拟合，利用Dropout在一定程度上面可以防止过拟合

![](https://images2015.cnblogs.com/blog/604152/201703/604152-20170330212327961-771136311.jpg)

如上图左，为没有Dropout的普通2层全连接结构，记为 r=a(Wv)，其中a为激活函数。

如上图右，为在第2层全连接后添加Dropout层的示意图。即在 模 型 训 练 时 随机让网络的某些节点不工作（输出置0），其它过程不变。

参考链接：
- [系列解读Dropout](https://www.cnblogs.com/welhzh/p/6648613.html)


## 激活层

Sigmoid 函数：$S(x) = \frac{1}{1+e^{-x}}$

## 正则化层

批量正则化:

$$ y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta $$

## Triplet Loss

Triplet Loss 可以理解为三元损失，主要用于训练差异性较小的样本，如人脸等，Feed数据包括锚（Anchor）示例、正（Positive）示例、负（Negative）示例，通过优化锚示例与正示例的距离小于锚示例与负示例的距离，实现样本的相似性计算。

![](https://upload-images.jianshu.io/upload_images/749674-36e5052fe9c91275?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

目标函数如下：

$$L = max(\sum\limits_{i}^{N}\Vert f(a_i^p) - f(x_i^p))\Vert^2 - \Vert f(a_i^p) - f(x_i^n))\Vert^2 + \alpha , 0)$$

由目标函数可以看出:

当x_a与x_n之间的距离 < x_a与x_p之间的距离加$\alpha$时，[]内的值大于零，就会产生损失。

当x_a与x_n之间的距离 >= x_a与x_p之间的距离加$\alpha$时，损失为零。