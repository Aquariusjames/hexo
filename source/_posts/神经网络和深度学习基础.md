---
layout: java
title: 神经网络和深度学习基础
date: 2018-10-30 14:24:37
tags: 
  - deeplearning
  - 深度学习
mathjax: true
---

# 符号定义

对于二分类的问题，假设输出$y$的值域为$\{0,1\}$, 输入的样本的数量为m，训练样本表示如下：
$$\{(x^{(1)}, x^{(2)}, ... , x^{(m)}\}$$

假设每个样本的特征向量为n维， 将每个样本作为列向量，按照行方向堆叠，得到一个$n \cdot m$的矩阵, 其中$ X \in R^{n*m} $

$$X = \begin{bmatrix}
\vdots &\vdots &\vdots\\
x_1 & x_2 & x_3\\
\vdots & \vdots &\vdots \\
\end{bmatrix}$$

# 线性回归

根据输入的样本，寻找一个线性函数$\hat y = f(x)$ 使得 $\hat y$的值更加接近$y$，通常表示如下：
$$ \hat y = w^Tx+b $$ 但是这个值有可能比0或者1相差太多，因此需要一个激活函数$\sigma(z) = \frac{1}{1+e^{-z}}$ 来将结果拉伸到0 ~ 1 之间。

如下图：
![](/images/dl/logistic.png)

修正之后有：

$$ \hat y = \sigma(w^Tx+b) $$

## 梯度下降（Gradient Descent）

如何求取w和b，使得$\hat y$的值更加接近于$y$？
采用梯度下降法来更新w和b，最终使得$\hat y$更加接近于$y$

$ \hat y = \sigma(w^Tx+b) $  -- 回归函数
$ \sigma(z) = \frac{1}{1+e^{-z}} $ -- 激活函数
$ L(\hat y, y) = -(y \log \hat y + (1-y)\log (1 - \hat y)) $ -- 损失函数
$ J(w,b) = \frac{1}{m} \sum\limits_{i = 1 }^m L(\hat y^{(i)} + y{(i)})$ -- 代价函数

有理论证明J(w,b)是关于w和b的凸函数，因此直接将J对w和b分别求导，令导数等于0，则得到的w和b即为所求的值，是的代价函数的值最小。

## 计算图

计算图的概念比较好理解，类似于链式求导法则，前向计算，反向求导，后面一步总是可以利用前面一步的结果

下面来计算线性回归中的求导部分：

公式如下:
$$z = w^Tx + b$$
$$\hat y = a = \sigma(z) = \frac{1}{1+e^{-z}}$$
$$L(a,y) = -(y\log a + (1-y)log(1-a))$$

这里令n=2,（假设输入的样本为二维向量）

则有前向计算公式:

1. $ z = w_1 \cdot x_1 + w_2 \cdot x_1 + b $
2. $a = \sigma (z)$
3. $ L(a,y)$

反向求导公式：
1. $ da = \frac{\partial L(a,y)}{\partial a}$
2. $ dz= \frac{\partial L(a,y)}{\partial z} \\ = \frac{\partial L(a,y)}{\partial a} \cdot  \frac{\partial a}{\partial z} \\ = (- \frac {y}{a} + \frac{1-y}{1-a}) \cdot a \cdot (1-y) \\ = a-y$
3. $dw = \frac{\partial L(a,y)}{\partial w} \\ = dz \cdot \frac{\partial z}{\partial w} \\ = dz \cdot x $
$=> dw_1 = x_1 \cdot dz, dw_2 = x_2 \cdot dz, dw_3 = x_3 \cdot dz$
4. $db = \frac{\partial L(a,y)}{\partial b} \\=dz \cdot \frac{\partial z}{\partial b} \\=dz$

更新方法：

$ w_1:= w_1 - \alpha \cdot dw_1 $
$ w_2:= w_2 - \alpha \cdot dw_2 $
$ b:= b - \alpha \cdot db $

考虑m个样本的话，将损失函数替换为代价函数，然后求均值即可。

代价函数 = avg(损失函数)

$\frac{\partial J(w,b)}{\partial w}  = \frac{1}{m} \sum\limits_{i = 1 }^m \frac{\partial L(a^{(i)} + y{(i)})}{\partial w}$

$ w:= w - \alpha \cdot \frac{\partial{J(w,b)}}{\partial{w}} $
$ b:= b - \alpha \cdot \frac{\partial{J(w,b)}}{\partial{b}} $

## 算法伪代码

initialize $J = 0, dw_1 = 0, dw_2 = 0, db = 0$
for i = 1 to m:
    $z^{(i)}=w^T \cdot x^{(i)} + b$
    $J += -(y^{(i)} \log a^{(i)} + (1-y)\log (1 - a^{(i)}))$
    $dz^{(i)} = a^{(i)} - y^{(i)}$
    $dw_1 += x_1^i \cdot dz^{(i)}$    
    $dw_2 += x_2^i \cdot dz^{(i)}$
    $db += dz^{(i)}$
end for
$J /= m$
$dw1 /= m$
$dw2 /= m$
$db /= m$
$w1 -= \alpha \cdot dw_1 $
$w2 -= \alpha \cdot dw_2 $
$b -= \alpha \cdot db $

## python 实现

``` python 
x = np.array([[3,4],[2,5],[6,7]])
x = np.transpose(x)
y = np.array([1,1,0]).reshape(3,1)
J = 0
m = y.shape[0]
w = np.zeros([2,1])
dw = np.zeros([2,1])
db = np.zeros([3,1])
b = np.zeros([3,1])
epoch = 100
lr = 0.001 # 学习率
for i in range(0, epoch):
    z = np.dot(w.T,x) + b.T
    a = 1/(1+np.exp(-z))
    J += np.sum(-(y*np.log(a)+(1-y)*np.log(1-a)))
    dz = a.T - y
    dw += np.dot(x, dz)
    db += dz
    dw /= m
    db /= m
    J /= m
    print('epoch: {} Loss = {}'.format(i, J))
    # 更新权值
    w -= lr*dw
    b -= lr*db
```

