---
title: 大数据平台维护笔记
tags:
  - 大数据
  - hadoop
categories:
  - 笔记
  - hadoop
date: 2017-06-09 12:47:59
---

## 1.hdfs文件迁移方法：
参考链接：
http://blog.csdn.net/bigkeen/article/details/51034902
http://www.cnblogs.com/juncaoit/p/6178747.html
http://blog.csdn.net/weipanp/article/details/42713149
- 可以相互通信的两个集群
在老的hadoop集群中开启yarn和hdfs服务，新的集群开启hdfs服务即可
执行以下命令：

``` bash
hadoop distcp -i hftp://old cluster ip:50070/src directory hdfs://192.168.91.130:8020/new cluster directory
```
- 删除老集群的hdfs文件
使用hdfs命令删除文件后执行
hadoop dfs -expunge
删除对应的DataNode和namenode路径下的文件夹

### 2.多次格式化NameNode导致无法启动DataNode的解决方案
http://www.cnblogs.com/yeahwell/p/5642798.html

### 3.查看某端口被映射到端口上
iptables -t nat -L -n  | grep 80
iptables -t nat --list   //检查nat列表信息
iptables -t nat -D PREROUTING 1    /Nat列表信息删除：序号从1 开始，后边以此+1.

参考链接：http://blog.csdn.net/xin_yu_xin/article/details/46416101

### 4.安装mysql
http://blog.csdn.net/chenpy/article/details/50344085

### 5.storm 并行度
rebalance操作：http://blog.csdn.net/jmppok/article/details/17243857
storm并行度理解：http://www.cnblogs.com/catkins/p/5254377.html

### 6.自定义系统
定制Ubuntu镜像：https://www.zybuluo.com/fanisfun/note/802272

### 7.cloudera-agent 启动报错解决方法
错误1：ERROR    Failed to connect to previous supervisor.
通常是由于之前已经启动了agent残留下来的进程产生的影响，因此需要将之前的进程清除掉
执行命令

    kill -9 $(pgrep -f supervisord)
    然后重启，除了第一次安装要求复制cloudera-agent到系统/etc/init.d/目录下，建议放置到rc.local文件启动吧
